Our final project builds upon the foundational architecture outlined in the Midterm Report by comparatively analyzing three distinct approaches to developing a poker-playing AI system: (1) reinforcement learning combined with search (RL+Search), (2) a fine-tuned large language model (LLM), specifically DeepSeek, and (3) two additional machine learning methods—AdaBoost decision trees and reinforcement learning using Long Short-Term Memory (LSTM) networks. Each method was assessed independently to identify their strategic strengths, limitations, and contextual applicability in the complex environment of heads-up no-limit Texas Hold’em poker.

The RL+Search framework served as the primary baseline approach. Utilizing the AlphaNLHoldem codebase, this method leverages simulated self-play and action rollouts to explore potential future game states. Through extensive simulations similar to Monte Carlo Tree Search (MCTS), the RL+Search approach evaluates the expected value of different moves, providing a robust, probabilistic strategy resilient to deceptive play by opponents. The strength of this approach lies in its ability to deeply explore tactical scenarios through exhaustive state-space evaluation, though it lacks intuitive strategic reasoning.
The second approach evaluated was DeepSeek, a large language model fine-tuned specifically on poker scenarios. This method employs reinforcement learning from human feedback (RLHF) and carefully crafted prompts to analyze complex poker situations involving betting history, community cards, stack sizes, and prior actions. DeepSeek’s primary advantage is its ability to generate nuanced strategic insights that capture subtle aspects of human-like reasoning, such as interpreting opponent behavior for potential bluffing or trapping. However, this method may falter when precise probabilistic simulation is required, especially under strict computational constraints.

Finally, we explored ensemble-based and temporal machine learning methods. The AdaBoost algorithm, utilizing decision trees as weak learners, provided quick, interpretable predictions based on engineered features such as round stage, pot size, and historical player actions. AdaBoost excelled in providing immediate and transparent decisions but struggled in complex, nuanced scenarios due to limited contextual reasoning capabilities. In contrast, the LSTM network approach effectively captured sequential dependencies and opponent behavioral patterns over multiple hands, making it highly suitable for adapting strategies across extended matches. Nonetheless, its predictions could be less accurate in isolated or novel scenarios.

By independently analyzing these approaches, our project highlighted critical insights into their respective contributions and limitations. The RL+Search approach offered robust tactical simulations, DeepSeek provided human-like strategic reasoning, AdaBoost delivered rapid and transparent decision-making, and LSTM enabled strategic adaptation through temporal modeling. Understanding these comparative strengths and weaknesses allows us to better appreciate the value and potential application contexts of each method in developing an effective poker-playing AI system.
